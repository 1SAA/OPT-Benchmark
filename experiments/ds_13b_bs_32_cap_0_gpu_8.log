WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
08/17/2022 21:19:22 - INFO - __main__ - Distributed environment: DEEPSPEED  Backend: nccl
Num processes: 8
Process index: 6
Local process index: 6
Device: cuda:6
ds_config: {'train_batch_size': 256, 'train_micro_batch_size_per_gpu': 32, 'gradient_accumulation_steps': 1, 'zero_optimization': {'stage': 3, 'offload_optimizer': {'device': 'cpu'}, 'offload_param': {'device': 'cpu'}, 'stage3_gather_16bit_weights_on_model_save': False}, 'steps_per_print': inf, 'fp16': {'enabled': True, 'initial_scale_power': 10}, 'zero_allow_untested_optimizer': True}

08/17/2022 21:19:22 - INFO - __main__ - Distributed environment: DEEPSPEED  Backend: nccl
Num processes: 8
Process index: 5
Local process index: 5
Device: cuda:5
ds_config: {'train_batch_size': 256, 'train_micro_batch_size_per_gpu': 32, 'gradient_accumulation_steps': 1, 'zero_optimization': {'stage': 3, 'offload_optimizer': {'device': 'cpu'}, 'offload_param': {'device': 'cpu'}, 'stage3_gather_16bit_weights_on_model_save': False}, 'steps_per_print': inf, 'fp16': {'enabled': True, 'initial_scale_power': 10}, 'zero_allow_untested_optimizer': True}

08/17/2022 21:19:22 - INFO - __main__ - Distributed environment: DEEPSPEED  Backend: nccl
Num processes: 8
Process index: 2
Local process index: 2
Device: cuda:2
ds_config: {'train_batch_size': 256, 'train_micro_batch_size_per_gpu': 32, 'gradient_accumulation_steps': 1, 'zero_optimization': {'stage': 3, 'offload_optimizer': {'device': 'cpu'}, 'offload_param': {'device': 'cpu'}, 'stage3_gather_16bit_weights_on_model_save': False}, 'steps_per_print': inf, 'fp16': {'enabled': True, 'initial_scale_power': 10}, 'zero_allow_untested_optimizer': True}

08/17/2022 21:19:22 - INFO - __main__ - Distributed environment: DEEPSPEED  Backend: nccl
Num processes: 8
Process index: 7
Local process index: 7
Device: cuda:7
ds_config: {'train_batch_size': 256, 'train_micro_batch_size_per_gpu': 32, 'gradient_accumulation_steps': 1, 'zero_optimization': {'stage': 3, 'offload_optimizer': {'device': 'cpu'}, 'offload_param': {'device': 'cpu'}, 'stage3_gather_16bit_weights_on_model_save': False}, 'steps_per_print': inf, 'fp16': {'enabled': True, 'initial_scale_power': 10}, 'zero_allow_untested_optimizer': True}

08/17/2022 21:19:22 - INFO - __main__ - Distributed environment: DEEPSPEED  Backend: nccl
Num processes: 8
Process index: 1
Local process index: 1
Device: cuda:1
ds_config: {'train_batch_size': 256, 'train_micro_batch_size_per_gpu': 32, 'gradient_accumulation_steps': 1, 'zero_optimization': {'stage': 3, 'offload_optimizer': {'device': 'cpu'}, 'offload_param': {'device': 'cpu'}, 'stage3_gather_16bit_weights_on_model_save': False}, 'steps_per_print': inf, 'fp16': {'enabled': True, 'initial_scale_power': 10}, 'zero_allow_untested_optimizer': True}

08/17/2022 21:19:22 - INFO - __main__ - Distributed environment: DEEPSPEED  Backend: nccl
Num processes: 8
Process index: 0
Local process index: 0
Device: cuda:0
ds_config: {'train_batch_size': 256, 'train_micro_batch_size_per_gpu': 32, 'gradient_accumulation_steps': 1, 'zero_optimization': {'stage': 3, 'offload_optimizer': {'device': 'cpu'}, 'offload_param': {'device': 'cpu'}, 'stage3_gather_16bit_weights_on_model_save': False}, 'steps_per_print': inf, 'fp16': {'enabled': True, 'initial_scale_power': 10}, 'zero_allow_untested_optimizer': True}

08/17/2022 21:19:22 - INFO - __main__ - Distributed environment: DEEPSPEED  Backend: nccl
Num processes: 8
Process index: 4
Local process index: 4
Device: cuda:4
ds_config: {'train_batch_size': 256, 'train_micro_batch_size_per_gpu': 32, 'gradient_accumulation_steps': 1, 'zero_optimization': {'stage': 3, 'offload_optimizer': {'device': 'cpu'}, 'offload_param': {'device': 'cpu'}, 'stage3_gather_16bit_weights_on_model_save': False}, 'steps_per_print': inf, 'fp16': {'enabled': True, 'initial_scale_power': 10}, 'zero_allow_untested_optimizer': True}

08/17/2022 21:19:22 - INFO - __main__ - Distributed environment: DEEPSPEED  Backend: nccl
Num processes: 8
Process index: 3
Local process index: 3
Device: cuda:3
ds_config: {'train_batch_size': 256, 'train_micro_batch_size_per_gpu': 32, 'gradient_accumulation_steps': 1, 'zero_optimization': {'stage': 3, 'offload_optimizer': {'device': 'cpu'}, 'offload_param': {'device': 'cpu'}, 'stage3_gather_16bit_weights_on_model_save': False}, 'steps_per_print': inf, 'fp16': {'enabled': True, 'initial_scale_power': 10}, 'zero_allow_untested_optimizer': True}


  0%|          | 0/3 [00:00<?, ?it/s]
100%|██████████| 3/3 [00:00<00:00, 362.13it/s]
08/17/2022 21:21:09 - WARNING - datasets.load - Using the latest cached version of the module from /home/lclsg/.cache/huggingface/modules/datasets_modules/datasets/wikitext/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126 (last modified on Wed Mar  9 15:40:35 2022) since it couldn't be found locally at wikitext., or remotely on the Hugging Face Hub.
08/17/2022 21:21:09 - WARNING - datasets.builder - Reusing dataset wikitext (/home/lclsg/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)

  0%|          | 0/3 [00:00<?, ?it/s]
100%|██████████| 3/3 [00:00<00:00, 673.89it/s]
loading configuration file https://huggingface.co/facebook/opt-13b/resolve/main/config.json from cache at /home/lclsg/.cache/huggingface/transformers/510eacb7ead22e89cbf85cad799bacfa07db8e509968fbfe2ac63e7f485c4b9e.174c2815b31cd5762c82327dc866c1f5eba318e6923cf00d01a7d63cbae5295a
Model config OPTConfig {
  "_name_or_path": "facebook/opt-13b",
  "_remove_final_layer_norm": false,
  "activation_dropout": 0.0,
  "activation_function": "relu",
  "architectures": [
    "OPTForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 2,
  "do_layer_norm_before": true,
  "dropout": 0.1,
  "eos_token_id": 2,
  "ffn_dim": 20480,
  "hidden_size": 5120,
  "init_std": 0.02,
  "layerdrop": 0.0,
  "max_position_embeddings": 2048,
  "model_type": "opt",
  "num_attention_heads": 40,
  "num_hidden_layers": 40,
  "output_projection": true,
  "pad_token_id": 1,
  "prefix": "</s>",
  "torch_dtype": "float16",
  "transformers_version": "4.21.0",
  "use_cache": true,
  "vocab_size": 50272,
  "word_embed_proj_dim": 5120
}

loading file https://huggingface.co/facebook/opt-13b/resolve/main/vocab.json from cache at /home/lclsg/.cache/huggingface/transformers/f787fefd101e3872c26b37c8decdfe86e02ac34b243c0ffcd5573cd2e632decb.647b4548b6d9ea817e82e7a9231a320231a1c9ea24053cc9e758f3fe68216f05
loading file https://huggingface.co/facebook/opt-13b/resolve/main/merges.txt from cache at /home/lclsg/.cache/huggingface/transformers/fd5aedb98f39796496dc996e498d49ca7b95f13e6b87aba5c77d31591fd47d9e.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b
loading file https://huggingface.co/facebook/opt-13b/resolve/main/added_tokens.json from cache at None
loading file https://huggingface.co/facebook/opt-13b/resolve/main/special_tokens_map.json from cache at /home/lclsg/.cache/huggingface/transformers/bd7b6b2b76d734fc30d0a3ef73907c1444885e5feef2a8bc69dfbe0bb86d5aec.c7cc7d24e97c79eaf304e87679fffb4f36cf739d549738da5cc604bf047de6ce
loading file https://huggingface.co/facebook/opt-13b/resolve/main/tokenizer_config.json from cache at /home/lclsg/.cache/huggingface/transformers/c43b0b5b3807b4df3e15ca678486682be47a558ea04ff3e4dfcd757744f571a4.cfcff7c8d43c19a6ed9075436f6d1f8149affb088ad7d2c5f4e57001b3c20a03

  0%|          | 0/3 [00:00<?, ?it/s]
100%|██████████| 3/3 [00:00<00:00, 621.59it/s]

  0%|          | 0/3 [00:00<?, ?it/s]
100%|██████████| 3/3 [00:00<00:00, 663.34it/s]

  0%|          | 0/3 [00:00<?, ?it/s]
100%|██████████| 3/3 [00:00<00:00, 614.13it/s]

  0%|          | 0/3 [00:00<?, ?it/s]
100%|██████████| 3/3 [00:00<00:00, 464.90it/s]

  0%|          | 0/3 [00:00<?, ?it/s]
100%|██████████| 3/3 [00:00<00:00, 822.68it/s]
08/17/2022 21:25:12 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lclsg/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-a3522cf6cea8d461.arrow
08/17/2022 21:25:12 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lclsg/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-283e1f8f02b97ee7.arrow
08/17/2022 21:25:13 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lclsg/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-96578d2791555270.arrow

  0%|          | 0/3 [00:00<?, ?it/s]
100%|██████████| 3/3 [00:00<00:00, 893.10it/s]
08/17/2022 21:30:28 - WARNING - __main__ - The tokenizer picked seems to have a very large `model_max_length` (1000000000000000019884624838656). Picking 1024 instead. You can change that default value by passing --block_size xxx.
08/17/2022 21:30:28 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lclsg/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-2708ad1a76e8c686.arrow
08/17/2022 21:30:28 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lclsg/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-2be098545fe6da5b.arrow
08/17/2022 21:30:28 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lclsg/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-5bf05cbc1659f76a.arrow
08/17/2022 21:30:30 - INFO - __main__ - Sample 608 of the training set: {'input_ids': [3184, 2156, 17792, 189, 3294, 227, 484, 1330, 6609, 150, 51, 3008, 49, 8047, 479, 96, 5, 5377, 9, 209, 12628, 37060, 1097, 11217, 2156, 209, 31402, 17174, 50, 2677, 36359, 1766, 6609, 32, 2333, 4997, 7, 25, 22, 18627, 1635, 22, 2156, 8, 28672, 227, 106, 32, 373, 18627, 5033, 1022, 479, 5598, 1022, 32, 747, 26914, 30, 5, 17014, 9, 10, 44017, 28807, 7, 41, 32834, 128, 29, 2171, 1082, 2156, 50, 5, 2166, 976, 9, 5, 8276, 14, 33439, 11, 4747, 4758, 45253, 479, 96, 2472, 17792, 67, 10946, 21875, 11, 3184, 149, 17210, 35911, 8, 5, 7329, 19, 97, 20237, 479, 1437, 50118, 2, 1698, 859, 1344, 64, 28, 6296, 2368, 6408, 88, 130, 1049, 4050, 2156, 61, 42407, 19, 6097, 31402, 17174, 6609, 4832, 37544, 8244, 17792, 2156, 19961, 21233, 17792, 2156, 8, 34194, 17792, 479, 11206, 70, 37544, 8244, 17792, 32, 45132, 8, 171, 32, 35309, 479, 28174, 21233, 17792, 32, 747, 9825, 2156, 215, 25, 39248, 2156, 5, 538, 7681, 9, 4686, 2088, 11576, 2156, 50, 34831, 16779, 2156, 5, 8276, 7681, 9, 2549, 8, 20941, 479, 20207, 3809, 1728, 17792, 747, 1807, 25, 37574, 50, 694, 6237, 13, 13744, 50, 1340, 20237, 7, 1323, 149, 5, 3551, 34194, 479, 1437, 50118, 2, 83, 780, 403, 9, 6979, 4040, 4104, 32188, 18303, 3554, 624, 17792, 2156, 12101, 33855, 31, 514, 908, 8, 12113, 6061, 49, 308, 33229, 2156, 32, 373, 36410, 26573, 479, 1437, 50118, 2, 2, 5457, 5457, 5457, 32732, 8964, 5457, 5457, 5457, 1437, 50118, 2, 2, 25087, 154, 5, 31402, 17174, 3184, 9, 10, 8276, 2156, 50, 5, 2677, 36359, 1766, 3184, 9, 63, 34003, 2156, 64, 694, 505, 14885, 59, 141, 5, 8276, 14023, 63, 5043, 479, 9732, 14073, 6448, 9, 3184, 8964, 680, 1577, 787, 12, 1039, 33803, 34774, 10486, 8, 234, 12642, 19416, 39655, 16572, 2156, 258, 9, 61, 64, 2592, 335, 23, 21495, 3547, 479, 635, 2156, 234, 12642, 15491, 32, 441, 7, 694, 335, 31, 61, 10, 37105, 9, 21459, 227, 15029, 9, 33310, 64, 28, 2319, 2156, 8, 5, 507, 678, 18627, 1635, 13, 10, 8276, 32, 3030, 30, 15582, 10, 4472, 37639, 936, 479, 23722, 13744, 3258, 3222, 6646, 40899, 16, 10, 17836, 23554, 5448, 13, 14978, 5, 1374, 8276, 2764, 26941, 8, 18627, 5033, 1022, 528, 7, 11324, 50, 97, 8931, 479, 26762, 8244, 32325, 1001, 1809, 16, 277, 13154, 9205, 13, 13684, 3425, 47743, 787, 12, 1039, 5462, 1589, 47188, 787, 12, 1039, 16068, 3569, 15229, 9, 17792, 479, 19803, 3540, 17601, 2839, 40867, 16572, 16, 341, 7, 2592, 795, 787, 12, 1039, 3547, 9825, 335, 59, 182, 739, 8276, 34003, 2156, 217, 14525, 21717, 25606, 10, 17390, 684, 25, 35235, 34774, 10486, 64, 67, 2592, 239, 787, 12, 1039, 3547, 335, 11, 103, 1200, 2156, 941, 13, 80, 787, 12, 1039, 43654, 30936, 9, 34194, 17792, 479, 4856, 5202, 6609, 32, 2333, 24232, 11, 5, 34786, 5423, 788, 36, 221, 10842, 4839, 2156, 10, 13215, 577, 5799, 31, 61, 9825, 414, 59, 1583, 9, 17792, 64, 28, 4756, 11, 5, 1026, 9, 13142, 44871, 34721, 13, 349, 37113, 11, 5, 8276, 479, 1437, 50118, 2, 1876, 55, 10596, 26929, 32, 684, 87, 8276, 6609, 479, 7029, 2156, 5, 278, 9, 15960, 6609, 16, 21099, 1706, 17792, 14, 64, 28, 2773, 13849, 7, 5, 1274, 1552, 11, 1577, 787, 12, 1039, 33803, 34774, 10486, 2156, 65, 9, 5, 538, 3184, 8964, 6448, 479, 96, 1989, 2156, 37544, 8244, 17792, 32, 31074, 1365, 7, 34774, 2072, 11, 7094, 13, 1577, 787, 12, 1039, 33803, 34774, 10486, 479, 20207, 3809, 1728, 17792, 2156, 30, 5709, 2156, 32, 1202, 7, 34774, 2072, 8, 32, 223, 26716, 11, 5, 221, 10842, 479, 31307, 9799, 12358, 26382, 5287, 33, 3751, 7, 20687, 209, 28262, 30, 28210, 15582, 4915, 6609, 9, 538, 14789, 4050, 479, 34786, 3184, 16782, 6448, 2120, 7, 694, 10, 839, 9, 10846, 10, 27099, 3184, 13, 17792, 1060, 6609, 33, 45, 57, 9280, 2368, 3030, 479, 1437, 50118, 2, 2, 5457, 5457, 34399, 8047, 5457, 5457, 1437, 50118, 2, 2, 1698, 859, 1344, 32, 5, 834, 5552, 624, 5, 3551, 2156, 26, 7, 28, 3406, 66, 5, 5941, 17966, 30, 5, 335, 45320, 11, 14819, 479, 590, 5, 8219, 9, 1402, 3505, 9, 34062, 2156, 144, 97, 12243, 20237, 32, 3487, 43783, 4785, 2115, 61, 17792, 1760, 479, 1698, 859, 1344, 146, 62, 457, 5, 3841, 2408, 9, 41, 11274, 1843, 1725, 493, 31435, 3551, 2156, 9641, 97, 13418, 5638, 4104, 46285, 215, 25, 5708, 8, 34062, 146, 62, 129, 155, 7606, 8, 291, 7606, 2156, 4067, 479, 20, 278, 9, 17792, 2327, 11, 10, 1989, 3551, 50, 3551, 1907, 16, 684, 25, 63, 27067, 4399, 479, 1437, 50118, 2, 20, 834, 26293, 9, 17792, 14, 67, 2386, 49, 5544, 278, 9, 8047, 16, 49, 1460, 7, 23379, 97, 20237, 4010, 8, 18412, 479, 20, 976, 9, 5, 8276, 2149, 13, 17014, 277, 28807, 16, 684, 25, 5, 17014, 1082, 8, 16, 747, 10, 6943, 50, 22, 7524, 22, 15, 5, 22481, 4084, 479, 152, 17014, 1460, 16, 43219, 30, 5, 31402, 17174, 3184, 9, 5, 8276, 2156, 61, 19857, 5, 17014, 1082, 7524, 2156, 8, 30, 5, 4747, 3611, 9, 5, 3817, 37788, 27976, 128, 526, 9781, 479, 34786, 17014, 64, 28, 29106, 3229, 8, 2167, 25606, 13, 1246, 2156, 5, 14966, 261, 3964, 40871, 34496, 8276, 40361, 7, 1050, 5667, 118, 11575, 179, 19, 10, 2849, 787, 12, 1039, 24295, 33063, 19231, 14863, 41156, 5891, 36, 28696, 158, 42736, 379, 256, 4839, 53, 473, 45, 23379, 23, 70, 7, 63, 33795, 811, 9486, 8982, 15, 3865, 3175, 36, 8061, 112, 256, 4839, 479, 24495, 352, 3694, 4747, 1022, 215, 25, 5, 1285, 9, 10, 881, 39902, 333, 7, 10, 17014, 1784, 64, 2128, 37850, 7, 823, 7677, 17014, 25606, 13, 1246, 2156, 5, 37788, 5073, 462, 326, 43612, 45774, 4626, 3175, 2167, 7, 5, 37788, 10395, 7398, 833, 40846, 34244, 136, 5, 182, 1122, 526, 3206, 9, 5, 37788, 10395, 16, 4104, 3964, 833, 479, 1437, 50118, 2, 1698, 859, 1344, 64, 23379, 7, 97, 17792, 25, 157, 25, 7, 650, 787, 12, 1039, 28807], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [3184, 2156, 17792, 189, 3294, 227, 484, 1330, 6609, 150, 51, 3008, 49, 8047, 479, 96, 5, 5377, 9, 209, 12628, 37060, 1097, 11217, 2156, 209, 31402, 17174, 50, 2677, 36359, 1766, 6609, 32, 2333, 4997, 7, 25, 22, 18627, 1635, 22, 2156, 8, 28672, 227, 106, 32, 373, 18627, 5033, 1022, 479, 5598, 1022, 32, 747, 26914, 30, 5, 17014, 9, 10, 44017, 28807, 7, 41, 32834, 128, 29, 2171, 1082, 2156, 50, 5, 2166, 976, 9, 5, 8276, 14, 33439, 11, 4747, 4758, 45253, 479, 96, 2472, 17792, 67, 10946, 21875, 11, 3184, 149, 17210, 35911, 8, 5, 7329, 19, 97, 20237, 479, 1437, 50118, 2, 1698, 859, 1344, 64, 28, 6296, 2368, 6408, 88, 130, 1049, 4050, 2156, 61, 42407, 19, 6097, 31402, 17174, 6609, 4832, 37544, 8244, 17792, 2156, 19961, 21233, 17792, 2156, 8, 34194, 17792, 479, 11206, 70, 37544, 8244, 17792, 32, 45132, 8, 171, 32, 35309, 479, 28174, 21233, 17792, 32, 747, 9825, 2156, 215, 25, 39248, 2156, 5, 538, 7681, 9, 4686, 2088, 11576, 2156, 50, 34831, 16779, 2156, 5, 8276, 7681, 9, 2549, 8, 20941, 479, 20207, 3809, 1728, 17792, 747, 1807, 25, 37574, 50, 694, 6237, 13, 13744, 50, 1340, 20237, 7, 1323, 149, 5, 3551, 34194, 479, 1437, 50118, 2, 83, 780, 403, 9, 6979, 4040, 4104, 32188, 18303, 3554, 624, 17792, 2156, 12101, 33855, 31, 514, 908, 8, 12113, 6061, 49, 308, 33229, 2156, 32, 373, 36410, 26573, 479, 1437, 50118, 2, 2, 5457, 5457, 5457, 32732, 8964, 5457, 5457, 5457, 1437, 50118, 2, 2, 25087, 154, 5, 31402, 17174, 3184, 9, 10, 8276, 2156, 50, 5, 2677, 36359, 1766, 3184, 9, 63, 34003, 2156, 64, 694, 505, 14885, 59, 141, 5, 8276, 14023, 63, 5043, 479, 9732, 14073, 6448, 9, 3184, 8964, 680, 1577, 787, 12, 1039, 33803, 34774, 10486, 8, 234, 12642, 19416, 39655, 16572, 2156, 258, 9, 61, 64, 2592, 335, 23, 21495, 3547, 479, 635, 2156, 234, 12642, 15491, 32, 441, 7, 694, 335, 31, 61, 10, 37105, 9, 21459, 227, 15029, 9, 33310, 64, 28, 2319, 2156, 8, 5, 507, 678, 18627, 1635, 13, 10, 8276, 32, 3030, 30, 15582, 10, 4472, 37639, 936, 479, 23722, 13744, 3258, 3222, 6646, 40899, 16, 10, 17836, 23554, 5448, 13, 14978, 5, 1374, 8276, 2764, 26941, 8, 18627, 5033, 1022, 528, 7, 11324, 50, 97, 8931, 479, 26762, 8244, 32325, 1001, 1809, 16, 277, 13154, 9205, 13, 13684, 3425, 47743, 787, 12, 1039, 5462, 1589, 47188, 787, 12, 1039, 16068, 3569, 15229, 9, 17792, 479, 19803, 3540, 17601, 2839, 40867, 16572, 16, 341, 7, 2592, 795, 787, 12, 1039, 3547, 9825, 335, 59, 182, 739, 8276, 34003, 2156, 217, 14525, 21717, 25606, 10, 17390, 684, 25, 35235, 34774, 10486, 64, 67, 2592, 239, 787, 12, 1039, 3547, 335, 11, 103, 1200, 2156, 941, 13, 80, 787, 12, 1039, 43654, 30936, 9, 34194, 17792, 479, 4856, 5202, 6609, 32, 2333, 24232, 11, 5, 34786, 5423, 788, 36, 221, 10842, 4839, 2156, 10, 13215, 577, 5799, 31, 61, 9825, 414, 59, 1583, 9, 17792, 64, 28, 4756, 11, 5, 1026, 9, 13142, 44871, 34721, 13, 349, 37113, 11, 5, 8276, 479, 1437, 50118, 2, 1876, 55, 10596, 26929, 32, 684, 87, 8276, 6609, 479, 7029, 2156, 5, 278, 9, 15960, 6609, 16, 21099, 1706, 17792, 14, 64, 28, 2773, 13849, 7, 5, 1274, 1552, 11, 1577, 787, 12, 1039, 33803, 34774, 10486, 2156, 65, 9, 5, 538, 3184, 8964, 6448, 479, 96, 1989, 2156, 37544, 8244, 17792, 32, 31074, 1365, 7, 34774, 2072, 11, 7094, 13, 1577, 787, 12, 1039, 33803, 34774, 10486, 479, 20207, 3809, 1728, 17792, 2156, 30, 5709, 2156, 32, 1202, 7, 34774, 2072, 8, 32, 223, 26716, 11, 5, 221, 10842, 479, 31307, 9799, 12358, 26382, 5287, 33, 3751, 7, 20687, 209, 28262, 30, 28210, 15582, 4915, 6609, 9, 538, 14789, 4050, 479, 34786, 3184, 16782, 6448, 2120, 7, 694, 10, 839, 9, 10846, 10, 27099, 3184, 13, 17792, 1060, 6609, 33, 45, 57, 9280, 2368, 3030, 479, 1437, 50118, 2, 2, 5457, 5457, 34399, 8047, 5457, 5457, 1437, 50118, 2, 2, 1698, 859, 1344, 32, 5, 834, 5552, 624, 5, 3551, 2156, 26, 7, 28, 3406, 66, 5, 5941, 17966, 30, 5, 335, 45320, 11, 14819, 479, 590, 5, 8219, 9, 1402, 3505, 9, 34062, 2156, 144, 97, 12243, 20237, 32, 3487, 43783, 4785, 2115, 61, 17792, 1760, 479, 1698, 859, 1344, 146, 62, 457, 5, 3841, 2408, 9, 41, 11274, 1843, 1725, 493, 31435, 3551, 2156, 9641, 97, 13418, 5638, 4104, 46285, 215, 25, 5708, 8, 34062, 146, 62, 129, 155, 7606, 8, 291, 7606, 2156, 4067, 479, 20, 278, 9, 17792, 2327, 11, 10, 1989, 3551, 50, 3551, 1907, 16, 684, 25, 63, 27067, 4399, 479, 1437, 50118, 2, 20, 834, 26293, 9, 17792, 14, 67, 2386, 49, 5544, 278, 9, 8047, 16, 49, 1460, 7, 23379, 97, 20237, 4010, 8, 18412, 479, 20, 976, 9, 5, 8276, 2149, 13, 17014, 277, 28807, 16, 684, 25, 5, 17014, 1082, 8, 16, 747, 10, 6943, 50, 22, 7524, 22, 15, 5, 22481, 4084, 479, 152, 17014, 1460, 16, 43219, 30, 5, 31402, 17174, 3184, 9, 5, 8276, 2156, 61, 19857, 5, 17014, 1082, 7524, 2156, 8, 30, 5, 4747, 3611, 9, 5, 3817, 37788, 27976, 128, 526, 9781, 479, 34786, 17014, 64, 28, 29106, 3229, 8, 2167, 25606, 13, 1246, 2156, 5, 14966, 261, 3964, 40871, 34496, 8276, 40361, 7, 1050, 5667, 118, 11575, 179, 19, 10, 2849, 787, 12, 1039, 24295, 33063, 19231, 14863, 41156, 5891, 36, 28696, 158, 42736, 379, 256, 4839, 53, 473, 45, 23379, 23, 70, 7, 63, 33795, 811, 9486, 8982, 15, 3865, 3175, 36, 8061, 112, 256, 4839, 479, 24495, 352, 3694, 4747, 1022, 215, 25, 5, 1285, 9, 10, 881, 39902, 333, 7, 10, 17014, 1784, 64, 2128, 37850, 7, 823, 7677, 17014, 25606, 13, 1246, 2156, 5, 37788, 5073, 462, 326, 43612, 45774, 4626, 3175, 2167, 7, 5, 37788, 10395, 7398, 833, 40846, 34244, 136, 5, 182, 1122, 526, 3206, 9, 5, 37788, 10395, 16, 4104, 3964, 833, 479, 1437, 50118, 2, 1698, 859, 1344, 64, 23379, 7, 97, 17792, 25, 157, 25, 7, 650, 787, 12, 1039, 28807]}.
08/17/2022 21:30:30 - INFO - __main__ - Sample 1035 of the training set: {'input_ids': [5123, 5457, 5457, 5457, 1437, 50118, 2, 2, 163, 5870, 1742, 1018, 21, 1412, 11, 628, 15002, 30, 5, 1731, 241, 942, 479, 85, 1665, 25, 41, 490, 210, 13, 5, 1392, 9, 3057, 8, 518, 2156, 217, 2592, 8, 5296, 479, 572, 5, 386, 9, 5, 2366, 997, 2156, 5, 210, 21, 4875, 30, 1337, 9901, 1134, 2156, 54, 341, 24, 25, 10, 1542, 13, 49, 1414, 479, 3515, 18059, 30068, 257, 128, 29, 18234, 5000, 11, 1466, 2156, 18748, 13168, 23, 5, 210, 479, 840, 5090, 58, 25891, 1070, 2156, 2183, 960, 31, 6231, 8, 30625, 7, 745, 3183, 479, 287, 11, 5, 1079, 9, 5, 343, 2156, 22353, 102, 1742, 1018, 128, 29, 588, 2587, 3266, 33, 67, 7408, 14299, 479, 287, 9, 1014, 2156, 5, 400, 255, 6412, 23749, 933, 21, 22175, 66, 10, 3862, 11236, 12283, 23, 5, 210, 13, 68, 132, 787, 6, 1039, 12096, 228, 353, 479, 1437, 50118, 2, 96, 902, 777, 2156, 5, 1664, 625, 853, 942, 880, 18748, 23, 5, 660, 26641, 10939, 1018, 11, 5, 3600, 271, 344, 1176, 873, 1418, 479, 85, 21, 65, 9, 5, 1154, 1048, 11, 5, 343, 137, 3172, 159, 1414, 11, 5, 419, 4525, 29, 479, 96, 772, 777, 2156, 5, 8185, 1247, 4142, 14015, 5, 660, 26641, 10939, 7, 5, 285, 2156, 19, 503, 21291, 3009, 70, 1667, 9, 5, 210, 479, 767, 7, 5, 1664, 625, 853, 10316, 4702, 3287, 3356, 10225, 1614, 260, 22, 660, 1073, 523, 22, 2156, 5, 2122, 16, 122, 490, 13, 265, 8, 40, 3511, 19, 97, 2174, 1048, 479, 1437, 50118, 2, 2, 5457, 5457, 5457, 27260, 5457, 5457, 5457, 1437, 50118, 2, 2, 20, 4722, 21271, 22205, 3536, 16, 41, 3222, 21494, 24555, 809, 13, 5, 21271, 2777, 11, 5, 10091, 976, 479, 96, 644, 570, 2156, 270, 9, 12491, 11551, 11878, 8097, 424, 1906, 585, 14, 5, 14619, 21, 8735, 7, 28, 16449, 11, 14949, 19, 5, 3233, 9, 8743, 1452, 995, 118, 8, 13934, 479, 3687, 5, 1768, 1377, 21, 5, 1663, 9, 10, 92, 4351, 13, 5, 3536, 11, 18059, 30068, 257, 2156, 11, 4972, 9, 12491, 128, 29, 2065, 737, 25, 5, 1312, 13, 5, 709, 8, 6174, 9, 5, 21271, 2777, 479, 96, 902, 570, 2156, 5, 4811, 7326, 13, 5, 92, 4722, 21271, 22205, 3536, 21, 4142, 4976, 23, 41, 9714, 2844, 11, 5, 343, 479, 1437, 50118, 2, 2, 5457, 5457, 5457, 6003, 2507, 5457, 5457, 5457, 1437, 50118, 2, 2, 18059, 30068, 257, 34, 10, 346, 9, 7336, 2156, 144, 9, 61, 58, 682, 11236, 479, 20, 343, 128, 29, 171, 3357, 4553, 24879, 1626, 2156, 867, 8, 758, 435, 1138, 32, 566, 209, 22883, 128, 1049, 916, 479, 598, 972, 5, 1197, 1077, 2156, 2303, 4844, 33, 67, 5812, 6051, 11, 758, 539, 14041, 2156, 215, 25, 5, 1327, 5085, 2830, 7665, 479, 1437, 50118, 2, 3687, 5, 92, 7336, 16, 5, 411, 1929, 32427, 5928, 5085, 479, 85, 21, 1490, 11, 1824, 8, 4142, 1357, 11, 1125, 479, 18488, 14133, 624, 10, 2993, 119, 29943, 9, 5, 24215, 1614, 2794, 1016, 4414, 2156, 24, 34, 10, 1510, 787, 12, 1039, 929, 2148, 19, 10, 1510, 7606, 22526, 731, 479, 20, 2303, 3352, 7, 1482, 81, 112, 787, 6, 1039, 12096, 3074, 30, 570, 2156, 13, 61, 24, 708, 7, 12558, 10, 2514, 1374, 745, 8, 1019, 2644, 479, 83, 92, 21486, 2303, 624, 5, 3062, 1495, 16, 67, 8735, 7, 28, 2121, 30, 5, 253, 9, 5, 76, 479, 1437, 50118, 2, 1944, 7336, 11, 5, 343, 680, 5, 27149, 1694, 3892, 5928, 5085, 2156, 1918, 3578, 8834, 5085, 2156, 16585, 16834, 5085, 2156, 5085, 35064, 787, 12, 1039, 16128, 462, 1630, 2156, 32719, 5085, 2156, 5085, 2646, 7590, 196, 2156, 5085, 840, 12705, 2156, 7774, 5085, 2156, 1586, 260, 22809, 446, 2156, 256, 4989, 5085, 2156, 5085, 21802, 3463, 2156, 5085, 20232, 9169, 1343, 2156, 1664, 625, 853, 5085, 2156, 9175, 5085, 2156, 13857, 11984, 5085, 2156, 28798, 5085, 24921, 415, 2156, 1053, 3631, 5085, 2156, 28798, 22809, 3138, 8, 15495, 4110, 5085, 479, 1437, 50118, 2, 2, 5457, 5457, 3061, 5457, 5457, 1437, 50118, 2, 2, 18059, 30068, 257, 16, 184, 7, 10, 346, 9, 8447, 1168, 11599, 3353, 479, 287, 233, 9, 5, 168, 128, 29, 4879, 14573, 586, 2156, 727, 1304, 420, 5, 812, 32, 1768, 7, 28, 17880, 6555, 8, 14015, 479, 1437, 50118, 2, 20, 21271, 496, 589, 36, 13687, 791, 4839, 21, 2885, 11, 5, 9323, 29, 2156, 148, 5, 17464, 4128, 675, 479, 96, 14757, 2156, 63, 8864, 8, 2644, 58, 4939, 479, 20, 13687, 791, 2226, 81, 5, 220, 291, 107, 88, 41, 21734, 6786, 9, 723, 2239, 2156, 19, 508, 6522, 2156, 7417, 813, 8, 81, 379, 787, 6, 1039, 12096, 521, 479, 374, 501, 759, 1014, 2156, 5, 7062, 12008, 2033, 10, 752, 168, 563, 7, 13490, 5, 21271, 496, 589, 2156, 61, 56, 57, 1367, 159, 11, 5, 419, 4525, 29, 479, 20, 17880, 6234, 3893, 701, 382, 68, 155, 787, 4, 1039, 231, 153, 2156, 8, 21, 2121, 11, 830, 777, 479, 1437, 50118, 2, 18059, 30068, 257, 589, 36, 24099, 4839, 16, 10, 786, 787, 12, 1039, 22463, 2737, 14, 16, 22635, 30, 10, 1785, 9, 3101, 5421, 8, 10, 589, 1080, 479, 85, 16, 5, 2900, 14069, 9, 10, 346, 9, 18341, 31, 5, 21271, 496, 589, 25, 157, 25, 97, 21271, 37306, 479, 6513, 16325, 30, 5, 2715, 2717, 788, 11, 344, 13093, 895, 2156, 2030, 3466, 2156, 25, 157, 25, 97, 12125, 3353, 2156, 5, 2737, 3948, 2213, 9, 11295, 31, 63, 707, 43631, 2156, 103, 9, 2661, 535, 15, 7, 5445, 6935, 128, 29, 4176, 5358, 2446, 7, 10, 10560, 3020, 479, 18059, 30068, 257, 589, 34, 2885, 8670, 19, 484, 97, 5286, 3353, 2156, 217, 5, 589, 9, 83, 337, 28659, 11, 10060, 2156, 130, 6630, 11, 5028, 2156, 707, 6630, 11, 6312, 2156, 5, 589, 9, 8743, 1452, 995, 118, 2156, 8, 80, 6630, 11, 6519, 479, 287, 9, 1125, 2156, 24099, 67, 34, 7678, 23890, 19, 5, 1785, 9, 5, 3870, 21494, 6481, 12714, 791, 479, 1437], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [5123, 5457, 5457, 5457, 1437, 50118, 2, 2, 163, 5870, 1742, 1018, 21, 1412, 11, 628, 15002, 30, 5, 1731, 241, 942, 479, 85, 1665, 25, 41, 490, 210, 13, 5, 1392, 9, 3057, 8, 518, 2156, 217, 2592, 8, 5296, 479, 572, 5, 386, 9, 5, 2366, 997, 2156, 5, 210, 21, 4875, 30, 1337, 9901, 1134, 2156, 54, 341, 24, 25, 10, 1542, 13, 49, 1414, 479, 3515, 18059, 30068, 257, 128, 29, 18234, 5000, 11, 1466, 2156, 18748, 13168, 23, 5, 210, 479, 840, 5090, 58, 25891, 1070, 2156, 2183, 960, 31, 6231, 8, 30625, 7, 745, 3183, 479, 287, 11, 5, 1079, 9, 5, 343, 2156, 22353, 102, 1742, 1018, 128, 29, 588, 2587, 3266, 33, 67, 7408, 14299, 479, 287, 9, 1014, 2156, 5, 400, 255, 6412, 23749, 933, 21, 22175, 66, 10, 3862, 11236, 12283, 23, 5, 210, 13, 68, 132, 787, 6, 1039, 12096, 228, 353, 479, 1437, 50118, 2, 96, 902, 777, 2156, 5, 1664, 625, 853, 942, 880, 18748, 23, 5, 660, 26641, 10939, 1018, 11, 5, 3600, 271, 344, 1176, 873, 1418, 479, 85, 21, 65, 9, 5, 1154, 1048, 11, 5, 343, 137, 3172, 159, 1414, 11, 5, 419, 4525, 29, 479, 96, 772, 777, 2156, 5, 8185, 1247, 4142, 14015, 5, 660, 26641, 10939, 7, 5, 285, 2156, 19, 503, 21291, 3009, 70, 1667, 9, 5, 210, 479, 767, 7, 5, 1664, 625, 853, 10316, 4702, 3287, 3356, 10225, 1614, 260, 22, 660, 1073, 523, 22, 2156, 5, 2122, 16, 122, 490, 13, 265, 8, 40, 3511, 19, 97, 2174, 1048, 479, 1437, 50118, 2, 2, 5457, 5457, 5457, 27260, 5457, 5457, 5457, 1437, 50118, 2, 2, 20, 4722, 21271, 22205, 3536, 16, 41, 3222, 21494, 24555, 809, 13, 5, 21271, 2777, 11, 5, 10091, 976, 479, 96, 644, 570, 2156, 270, 9, 12491, 11551, 11878, 8097, 424, 1906, 585, 14, 5, 14619, 21, 8735, 7, 28, 16449, 11, 14949, 19, 5, 3233, 9, 8743, 1452, 995, 118, 8, 13934, 479, 3687, 5, 1768, 1377, 21, 5, 1663, 9, 10, 92, 4351, 13, 5, 3536, 11, 18059, 30068, 257, 2156, 11, 4972, 9, 12491, 128, 29, 2065, 737, 25, 5, 1312, 13, 5, 709, 8, 6174, 9, 5, 21271, 2777, 479, 96, 902, 570, 2156, 5, 4811, 7326, 13, 5, 92, 4722, 21271, 22205, 3536, 21, 4142, 4976, 23, 41, 9714, 2844, 11, 5, 343, 479, 1437, 50118, 2, 2, 5457, 5457, 5457, 6003, 2507, 5457, 5457, 5457, 1437, 50118, 2, 2, 18059, 30068, 257, 34, 10, 346, 9, 7336, 2156, 144, 9, 61, 58, 682, 11236, 479, 20, 343, 128, 29, 171, 3357, 4553, 24879, 1626, 2156, 867, 8, 758, 435, 1138, 32, 566, 209, 22883, 128, 1049, 916, 479, 598, 972, 5, 1197, 1077, 2156, 2303, 4844, 33, 67, 5812, 6051, 11, 758, 539, 14041, 2156, 215, 25, 5, 1327, 5085, 2830, 7665, 479, 1437, 50118, 2, 3687, 5, 92, 7336, 16, 5, 411, 1929, 32427, 5928, 5085, 479, 85, 21, 1490, 11, 1824, 8, 4142, 1357, 11, 1125, 479, 18488, 14133, 624, 10, 2993, 119, 29943, 9, 5, 24215, 1614, 2794, 1016, 4414, 2156, 24, 34, 10, 1510, 787, 12, 1039, 929, 2148, 19, 10, 1510, 7606, 22526, 731, 479, 20, 2303, 3352, 7, 1482, 81, 112, 787, 6, 1039, 12096, 3074, 30, 570, 2156, 13, 61, 24, 708, 7, 12558, 10, 2514, 1374, 745, 8, 1019, 2644, 479, 83, 92, 21486, 2303, 624, 5, 3062, 1495, 16, 67, 8735, 7, 28, 2121, 30, 5, 253, 9, 5, 76, 479, 1437, 50118, 2, 1944, 7336, 11, 5, 343, 680, 5, 27149, 1694, 3892, 5928, 5085, 2156, 1918, 3578, 8834, 5085, 2156, 16585, 16834, 5085, 2156, 5085, 35064, 787, 12, 1039, 16128, 462, 1630, 2156, 32719, 5085, 2156, 5085, 2646, 7590, 196, 2156, 5085, 840, 12705, 2156, 7774, 5085, 2156, 1586, 260, 22809, 446, 2156, 256, 4989, 5085, 2156, 5085, 21802, 3463, 2156, 5085, 20232, 9169, 1343, 2156, 1664, 625, 853, 5085, 2156, 9175, 5085, 2156, 13857, 11984, 5085, 2156, 28798, 5085, 24921, 415, 2156, 1053, 3631, 5085, 2156, 28798, 22809, 3138, 8, 15495, 4110, 5085, 479, 1437, 50118, 2, 2, 5457, 5457, 3061, 5457, 5457, 1437, 50118, 2, 2, 18059, 30068, 257, 16, 184, 7, 10, 346, 9, 8447, 1168, 11599, 3353, 479, 287, 233, 9, 5, 168, 128, 29, 4879, 14573, 586, 2156, 727, 1304, 420, 5, 812, 32, 1768, 7, 28, 17880, 6555, 8, 14015, 479, 1437, 50118, 2, 20, 21271, 496, 589, 36, 13687, 791, 4839, 21, 2885, 11, 5, 9323, 29, 2156, 148, 5, 17464, 4128, 675, 479, 96, 14757, 2156, 63, 8864, 8, 2644, 58, 4939, 479, 20, 13687, 791, 2226, 81, 5, 220, 291, 107, 88, 41, 21734, 6786, 9, 723, 2239, 2156, 19, 508, 6522, 2156, 7417, 813, 8, 81, 379, 787, 6, 1039, 12096, 521, 479, 374, 501, 759, 1014, 2156, 5, 7062, 12008, 2033, 10, 752, 168, 563, 7, 13490, 5, 21271, 496, 589, 2156, 61, 56, 57, 1367, 159, 11, 5, 419, 4525, 29, 479, 20, 17880, 6234, 3893, 701, 382, 68, 155, 787, 4, 1039, 231, 153, 2156, 8, 21, 2121, 11, 830, 777, 479, 1437, 50118, 2, 18059, 30068, 257, 589, 36, 24099, 4839, 16, 10, 786, 787, 12, 1039, 22463, 2737, 14, 16, 22635, 30, 10, 1785, 9, 3101, 5421, 8, 10, 589, 1080, 479, 85, 16, 5, 2900, 14069, 9, 10, 346, 9, 18341, 31, 5, 21271, 496, 589, 25, 157, 25, 97, 21271, 37306, 479, 6513, 16325, 30, 5, 2715, 2717, 788, 11, 344, 13093, 895, 2156, 2030, 3466, 2156, 25, 157, 25, 97, 12125, 3353, 2156, 5, 2737, 3948, 2213, 9, 11295, 31, 63, 707, 43631, 2156, 103, 9, 2661, 535, 15, 7, 5445, 6935, 128, 29, 4176, 5358, 2446, 7, 10, 10560, 3020, 479, 18059, 30068, 257, 589, 34, 2885, 8670, 19, 484, 97, 5286, 3353, 2156, 217, 5, 589, 9, 83, 337, 28659, 11, 10060, 2156, 130, 6630, 11, 5028, 2156, 707, 6630, 11, 6312, 2156, 5, 589, 9, 8743, 1452, 995, 118, 2156, 8, 80, 6630, 11, 6519, 479, 287, 9, 1125, 2156, 24099, 67, 34, 7678, 23890, 19, 5, 1785, 9, 5, 3870, 21494, 6481, 12714, 791, 479, 1437]}.
08/17/2022 21:30:30 - INFO - __main__ - Sample 1589 of the training set: {'input_ids': [50118, 2, 248, 7669, 22258, 254, 28411, 14, 289, 2726, 1999, 16, 5, 43792, 1258, 9, 5, 25105, 479, 91, 13119, 4619, 257, 128, 29, 1150, 8, 16420, 289, 2726, 1999, 2156, 3735, 4619, 257, 7, 4157, 289, 2726, 1999, 479, 287, 248, 7669, 22258, 254, 8, 4619, 257, 989, 11, 10, 7324, 2156, 289, 2726, 1999, 5741, 7, 2916, 106, 53, 10578, 25606, 3064, 88, 10, 8037, 2156, 37, 2419, 10, 583, 787, 12, 1039, 744, 676, 8, 25269, 59, 39, 375, 301, 11, 545, 3546, 479, 96, 14, 76, 2156, 31918, 6542, 3338, 16, 4568, 7, 34425, 5, 11673, 9, 121, 1208, 4147, 271, 77, 37, 23154, 9, 5, 10025, 25105, 229, 2331, 5371, 2456, 8604, 36, 289, 2726, 1999, 4839, 2156, 8501, 9, 5, 121, 1208, 4147, 625, 3835, 479, 1745, 16706, 4040, 3657, 128, 29, 1354, 2156, 38061, 6472, 2028, 1999, 35120, 36, 4619, 257, 4839, 2156, 6138, 5371, 2456, 8604, 2156, 53, 37, 3106, 1003, 124, 479, 1405, 11204, 2156, 9462, 1829, 705, 1585, 102, 36, 248, 7669, 22258, 254, 4839, 2156, 30864, 29, 71, 38061, 763, 8, 708, 10, 1465, 227, 1003, 8, 5371, 2456, 8604, 25606, 5, 1924, 40, 12908, 69, 8, 5, 22650, 40, 28, 41344, 31, 121, 1208, 4147, 625, 479, 5371, 2456, 8604, 128, 29, 1124, 3315, 7, 9462, 1829, 705, 128, 29, 2020, 16812, 479, 16706, 4040, 3657, 2156, 959, 2156, 16340, 5034, 14, 5371, 2456, 8604, 45, 12908, 39, 1354, 2156, 142, 5371, 2456, 8604, 34, 10, 239, 778, 9, 8180, 11, 2168, 2156, 8, 37, 473, 45, 2813, 7, 192, 38061, 763, 23772, 9725, 479, 3791, 6649, 2156, 5371, 853, 8604, 37210, 7, 5, 8453, 128, 29, 2069, 8, 3271, 11081, 7, 12908, 38061, 763, 2156, 1618, 69, 30719, 479, 1437, 50118, 2, 5371, 2456, 8604, 172, 1239, 38061, 763, 2156, 39, 3878, 2156, 8, 39, 575, 90, 6650, 7, 5, 5371, 2456, 1469, 677, 4488, 13271, 12619, 10, 15344, 7, 2639, 27481, 31, 1840, 479, 38061, 763, 4501, 5371, 2456, 8604, 8109, 39, 657, 13, 69, 479, 520, 37, 473, 45, 2519, 2156, 79, 12744, 2580, 5, 16996, 1964, 51, 33, 1146, 13, 5, 18829, 1910, 8, 2156, 634, 69, 308, 1925, 2156, 24995, 41, 2274, 15, 10, 3027, 3152, 9, 5371, 2456, 8604, 1618, 39, 1528, 657, 7, 109, 39, 4053, 479, 660, 1710, 9716, 8842, 7, 1137, 5371, 2456, 8604, 14, 9462, 1829, 705, 8, 6542, 3338, 128, 29, 3835, 33, 848, 16706, 4040, 3657, 8, 32, 122, 6404, 1706, 106, 479, 252, 5240, 3691, 2156, 8, 6542, 3338, 2019, 5371, 2456, 8604, 7, 2168, 39, 3878, 479, 5371, 2456, 8604, 34520, 14564, 5, 1539, 8, 10469, 10, 6317, 3878, 2156, 53, 16, 11166, 1710, 11, 5, 609, 479, 6542, 3338, 2156, 6889, 30, 5371, 2456, 8604, 128, 102, 24895, 2156, 34, 10, 464, 9, 1144, 479, 635, 2156, 9462, 1829, 705, 1388, 6666, 2156, 2140, 21354, 38061, 763, 11293, 2156, 53, 37, 16, 11, 1004, 848, 30, 5371, 2456, 8604, 479, 83, 8180, 38061, 763, 6990, 5371, 2456, 8604, 7, 23721, 39, 657, 2156, 53, 137, 37, 64, 2519, 2156, 79, 8524, 8, 5712, 160, 5, 15344, 479, 11281, 46509, 2156, 37, 3905, 69, 8, 5712, 7, 39, 308, 744, 479, 1437, 50118, 2, 572, 2239, 59, 39, 375, 301, 2156, 289, 2726, 1999, 16, 8262, 31, 5, 8037, 30, 10, 28783, 1440, 17455, 36, 54, 16, 5, 43792, 1258, 9, 6542, 3338, 4839, 8, 2156, 19, 4856, 16187, 128, 29, 244, 2156, 5695, 121, 1208, 4147, 625, 479, 91, 3077, 899, 7, 248, 7669, 22258, 254, 128, 29, 15653, 8, 27169, 2726, 5977, 4330, 2758, 248, 7669, 22258, 254, 14, 114, 4619, 257, 128, 29, 6180, 9, 5, 375, 32, 45, 19096, 624, 5, 183, 51, 64, 393, 28, 19096, 2156, 8, 79, 40, 28, 19, 248, 7669, 22258, 254, 6000, 479, 289, 2726, 1999, 24781, 7527, 4619, 257, 8, 1239, 69, 7, 5371, 2456, 1469, 677, 4488, 8, 11, 5, 609, 2156, 5977, 4330, 16, 13636, 848, 30, 248, 7669, 22258, 254, 479, 248, 7669, 22258, 254, 8842, 30, 7324, 8, 6990, 4619, 257, 7, 283, 19, 123, 25606, 959, 4619, 257, 3681, 38061, 763, 128, 29, 7674, 2156, 8, 69, 375, 787, 12, 1039, 301, 6180, 32, 19096, 25606, 79, 10854, 5110, 19, 289, 2726, 1999, 479, 289, 2726, 1999, 8333, 248, 7669, 22258, 254, 2156, 8, 19, 5, 244, 9, 17455, 2156, 10200, 7, 3549, 123, 479, 1437, 50118, 2, 2, 5457, 5457, 6719, 5457, 5457, 1437, 50118, 2, 2, 3513, 7275, 260, 25, 229, 2331, 5371, 2456, 8604, 8, 289, 2726, 1999, 479, 229, 2331, 5371, 2456, 8604, 16, 5, 8501, 787, 12, 1039, 11, 787, 12, 1039, 834, 9, 5, 121, 1208, 4147, 625, 3835, 54, 8524, 11, 2168, 11, 545, 3546, 479, 832, 43792, 1258, 2156, 289, 2726, 1999, 2156, 16, 10, 10667, 787, 12, 1039, 4806, 28782, 11, 2338, 2156, 54, 5712, 11, 657, 19, 4619, 257, 479, 520, 289, 2726, 1999, 28411, 39, 375, 301, 2156, 37, 7552, 4619, 257, 31, 69, 11204, 248, 7669, 22258, 254, 2156, 54, 848, 69, 1150, 8, 18699, 289, 2726, 1999, 25, 5, 28051, 479, 1437, 50118, 2, 229, 1176, 337, 14644, 271, 10163, 25, 38061, 6472, 8865, 35120, 8, 4619, 257, 479, 38061, 6472, 8865, 35120, 16, 5, 6420, 19169, 9, 121, 1208, 4147, 625, 54, 16, 11, 657, 19, 5371, 2456, 8604, 8, 67, 8524, 11, 545, 3546, 479, 96, 2338, 2156, 69, 43792, 1258, 16, 4619, 3578, 36, 29853, 4619, 257, 4839, 2156, 10, 575, 3743, 1294, 479, 264, 5712, 11, 657, 19, 289, 2726, 1999, 53, 2046, 123, 7, 28, 69, 1150, 128, 29, 28051, 142, 9, 248, 7669, 22258, 254, 128, 29, 29244, 479, 264, 10854, 5110, 19, 289, 2726, 1999, 11, 5, 253, 71, 21556, 69, 375, 301, 479, 1437, 50118, 2, 7439, 6452, 25, 9462, 1829, 705, 1585, 102, 8, 248, 7669, 22258, 254, 479, 9462, 1829, 705, 1585, 102, 16, 38061, 6472, 8865, 128, 29, 11204, 54, 30864, 29, 71, 69, 8, 16, 27064, 9, 5371, 2456, 8604, 479, 91, 2156, 71, 2429, 38061, 6472, 8865, 2156, 16, 848, 30, 5371, 2456, 8604, 479], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [50118, 2, 248, 7669, 22258, 254, 28411, 14, 289, 2726, 1999, 16, 5, 43792, 1258, 9, 5, 25105, 479, 91, 13119, 4619, 257, 128, 29, 1150, 8, 16420, 289, 2726, 1999, 2156, 3735, 4619, 257, 7, 4157, 289, 2726, 1999, 479, 287, 248, 7669, 22258, 254, 8, 4619, 257, 989, 11, 10, 7324, 2156, 289, 2726, 1999, 5741, 7, 2916, 106, 53, 10578, 25606, 3064, 88, 10, 8037, 2156, 37, 2419, 10, 583, 787, 12, 1039, 744, 676, 8, 25269, 59, 39, 375, 301, 11, 545, 3546, 479, 96, 14, 76, 2156, 31918, 6542, 3338, 16, 4568, 7, 34425, 5, 11673, 9, 121, 1208, 4147, 271, 77, 37, 23154, 9, 5, 10025, 25105, 229, 2331, 5371, 2456, 8604, 36, 289, 2726, 1999, 4839, 2156, 8501, 9, 5, 121, 1208, 4147, 625, 3835, 479, 1745, 16706, 4040, 3657, 128, 29, 1354, 2156, 38061, 6472, 2028, 1999, 35120, 36, 4619, 257, 4839, 2156, 6138, 5371, 2456, 8604, 2156, 53, 37, 3106, 1003, 124, 479, 1405, 11204, 2156, 9462, 1829, 705, 1585, 102, 36, 248, 7669, 22258, 254, 4839, 2156, 30864, 29, 71, 38061, 763, 8, 708, 10, 1465, 227, 1003, 8, 5371, 2456, 8604, 25606, 5, 1924, 40, 12908, 69, 8, 5, 22650, 40, 28, 41344, 31, 121, 1208, 4147, 625, 479, 5371, 2456, 8604, 128, 29, 1124, 3315, 7, 9462, 1829, 705, 128, 29, 2020, 16812, 479, 16706, 4040, 3657, 2156, 959, 2156, 16340, 5034, 14, 5371, 2456, 8604, 45, 12908, 39, 1354, 2156, 142, 5371, 2456, 8604, 34, 10, 239, 778, 9, 8180, 11, 2168, 2156, 8, 37, 473, 45, 2813, 7, 192, 38061, 763, 23772, 9725, 479, 3791, 6649, 2156, 5371, 853, 8604, 37210, 7, 5, 8453, 128, 29, 2069, 8, 3271, 11081, 7, 12908, 38061, 763, 2156, 1618, 69, 30719, 479, 1437, 50118, 2, 5371, 2456, 8604, 172, 1239, 38061, 763, 2156, 39, 3878, 2156, 8, 39, 575, 90, 6650, 7, 5, 5371, 2456, 1469, 677, 4488, 13271, 12619, 10, 15344, 7, 2639, 27481, 31, 1840, 479, 38061, 763, 4501, 5371, 2456, 8604, 8109, 39, 657, 13, 69, 479, 520, 37, 473, 45, 2519, 2156, 79, 12744, 2580, 5, 16996, 1964, 51, 33, 1146, 13, 5, 18829, 1910, 8, 2156, 634, 69, 308, 1925, 2156, 24995, 41, 2274, 15, 10, 3027, 3152, 9, 5371, 2456, 8604, 1618, 39, 1528, 657, 7, 109, 39, 4053, 479, 660, 1710, 9716, 8842, 7, 1137, 5371, 2456, 8604, 14, 9462, 1829, 705, 8, 6542, 3338, 128, 29, 3835, 33, 848, 16706, 4040, 3657, 8, 32, 122, 6404, 1706, 106, 479, 252, 5240, 3691, 2156, 8, 6542, 3338, 2019, 5371, 2456, 8604, 7, 2168, 39, 3878, 479, 5371, 2456, 8604, 34520, 14564, 5, 1539, 8, 10469, 10, 6317, 3878, 2156, 53, 16, 11166, 1710, 11, 5, 609, 479, 6542, 3338, 2156, 6889, 30, 5371, 2456, 8604, 128, 102, 24895, 2156, 34, 10, 464, 9, 1144, 479, 635, 2156, 9462, 1829, 705, 1388, 6666, 2156, 2140, 21354, 38061, 763, 11293, 2156, 53, 37, 16, 11, 1004, 848, 30, 5371, 2456, 8604, 479, 83, 8180, 38061, 763, 6990, 5371, 2456, 8604, 7, 23721, 39, 657, 2156, 53, 137, 37, 64, 2519, 2156, 79, 8524, 8, 5712, 160, 5, 15344, 479, 11281, 46509, 2156, 37, 3905, 69, 8, 5712, 7, 39, 308, 744, 479, 1437, 50118, 2, 572, 2239, 59, 39, 375, 301, 2156, 289, 2726, 1999, 16, 8262, 31, 5, 8037, 30, 10, 28783, 1440, 17455, 36, 54, 16, 5, 43792, 1258, 9, 6542, 3338, 4839, 8, 2156, 19, 4856, 16187, 128, 29, 244, 2156, 5695, 121, 1208, 4147, 625, 479, 91, 3077, 899, 7, 248, 7669, 22258, 254, 128, 29, 15653, 8, 27169, 2726, 5977, 4330, 2758, 248, 7669, 22258, 254, 14, 114, 4619, 257, 128, 29, 6180, 9, 5, 375, 32, 45, 19096, 624, 5, 183, 51, 64, 393, 28, 19096, 2156, 8, 79, 40, 28, 19, 248, 7669, 22258, 254, 6000, 479, 289, 2726, 1999, 24781, 7527, 4619, 257, 8, 1239, 69, 7, 5371, 2456, 1469, 677, 4488, 8, 11, 5, 609, 2156, 5977, 4330, 16, 13636, 848, 30, 248, 7669, 22258, 254, 479, 248, 7669, 22258, 254, 8842, 30, 7324, 8, 6990, 4619, 257, 7, 283, 19, 123, 25606, 959, 4619, 257, 3681, 38061, 763, 128, 29, 7674, 2156, 8, 69, 375, 787, 12, 1039, 301, 6180, 32, 19096, 25606, 79, 10854, 5110, 19, 289, 2726, 1999, 479, 289, 2726, 1999, 8333, 248, 7669, 22258, 254, 2156, 8, 19, 5, 244, 9, 17455, 2156, 10200, 7, 3549, 123, 479, 1437, 50118, 2, 2, 5457, 5457, 6719, 5457, 5457, 1437, 50118, 2, 2, 3513, 7275, 260, 25, 229, 2331, 5371, 2456, 8604, 8, 289, 2726, 1999, 479, 229, 2331, 5371, 2456, 8604, 16, 5, 8501, 787, 12, 1039, 11, 787, 12, 1039, 834, 9, 5, 121, 1208, 4147, 625, 3835, 54, 8524, 11, 2168, 11, 545, 3546, 479, 832, 43792, 1258, 2156, 289, 2726, 1999, 2156, 16, 10, 10667, 787, 12, 1039, 4806, 28782, 11, 2338, 2156, 54, 5712, 11, 657, 19, 4619, 257, 479, 520, 289, 2726, 1999, 28411, 39, 375, 301, 2156, 37, 7552, 4619, 257, 31, 69, 11204, 248, 7669, 22258, 254, 2156, 54, 848, 69, 1150, 8, 18699, 289, 2726, 1999, 25, 5, 28051, 479, 1437, 50118, 2, 229, 1176, 337, 14644, 271, 10163, 25, 38061, 6472, 8865, 35120, 8, 4619, 257, 479, 38061, 6472, 8865, 35120, 16, 5, 6420, 19169, 9, 121, 1208, 4147, 625, 54, 16, 11, 657, 19, 5371, 2456, 8604, 8, 67, 8524, 11, 545, 3546, 479, 96, 2338, 2156, 69, 43792, 1258, 16, 4619, 3578, 36, 29853, 4619, 257, 4839, 2156, 10, 575, 3743, 1294, 479, 264, 5712, 11, 657, 19, 289, 2726, 1999, 53, 2046, 123, 7, 28, 69, 1150, 128, 29, 28051, 142, 9, 248, 7669, 22258, 254, 128, 29, 29244, 479, 264, 10854, 5110, 19, 289, 2726, 1999, 11, 5, 253, 71, 21556, 69, 375, 301, 479, 1437, 50118, 2, 7439, 6452, 25, 9462, 1829, 705, 1585, 102, 8, 248, 7669, 22258, 254, 479, 9462, 1829, 705, 1585, 102, 16, 38061, 6472, 8865, 128, 29, 11204, 54, 30864, 29, 71, 69, 8, 16, 27064, 9, 5371, 2456, 8604, 479, 91, 2156, 71, 2429, 38061, 6472, 8865, 2156, 16, 848, 30, 5371, 2456, 8604, 479]}.
08/17/2022 21:30:30 - INFO - accelerate.accelerator - Since you passed both train and evaluation dataloader, `is_train_batch_min` (here True will decide the `train_batch_size` (8).
[2022-08-17 21:30:30,509] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed info: version=0.6.5, git-hash=unknown, git-branch=unknown
08/17/2022 21:30:50 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:2 to store for rank: 1
08/17/2022 21:30:51 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:2 to store for rank: 3
08/17/2022 21:30:52 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:2 to store for rank: 0
08/17/2022 21:30:52 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:2 to store for rank: 6
08/17/2022 21:30:52 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:2 to store for rank: 5
08/17/2022 21:30:55 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:2 to store for rank: 4
08/17/2022 21:30:58 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:2 to store for rank: 7
08/17/2022 21:31:00 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:2 (world_size=8, worker_count=7, timeout=0:30:00)
08/17/2022 21:31:00 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:2 to store for rank: 2
08/17/2022 21:31:00 - INFO - torch.distributed.distributed_c10d - Rank 2: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.
08/17/2022 21:31:00 - INFO - torch.distributed.distributed_c10d - Rank 3: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.
08/17/2022 21:31:00 - INFO - torch.distributed.distributed_c10d - Rank 4: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.
08/17/2022 21:31:00 - INFO - torch.distributed.distributed_c10d - Rank 1: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.
08/17/2022 21:31:00 - INFO - torch.distributed.distributed_c10d - Rank 5: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.
08/17/2022 21:31:00 - INFO - torch.distributed.distributed_c10d - Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.
08/17/2022 21:31:00 - INFO - torch.distributed.distributed_c10d - Rank 6: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.
08/17/2022 21:31:00 - INFO - torch.distributed.distributed_c10d - Rank 7: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.
[2022-08-17 21:31:02,473] [INFO] [engine.py:279:__init__] DeepSpeed Flops Profiler Enabled: False
[2022-08-17 21:31:02,473] [INFO] [engine.py:1087:_configure_optimizer] Removing param_group that has no 'params' in the client Optimizer
[2022-08-17 21:31:02,473] [INFO] [engine.py:1092:_configure_optimizer] Using client Optimizer as basic optimizer
[2022-08-17 21:31:02,506] [INFO] [engine.py:1109:_configure_optimizer] DeepSpeed Basic Optimizer = AdamW
[2022-08-17 21:31:02,506] [INFO] [utils.py:53:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>
[2022-08-17 21:31:02,507] [INFO] [logging.py:69:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer
[2022-08-17 21:31:02,507] [INFO] [engine.py:1410:_configure_zero_optimizer] Initializing ZeRO Stage 3
[2022-08-17 21:31:02,514] [INFO] [stage3.py:275:__init__] Reduce bucket size 500000000
[2022-08-17 21:31:02,514] [INFO] [stage3.py:276:__init__] Prefetch bucket size 50000000
Using /home/lclsg/.cache/torch_extensions/py37_cu113 as PyTorch extensions root...
Using /home/lclsg/.cache/torch_extensions/py37_cu113 as PyTorch extensions root...
Using /home/lclsg/.cache/torch_extensions/py37_cu113 as PyTorch extensions root...
Using /home/lclsg/.cache/torch_extensions/py37_cu113 as PyTorch extensions root...
Using /home/lclsg/.cache/torch_extensions/py37_cu113 as PyTorch extensions root...
Using /home/lclsg/.cache/torch_extensions/py37_cu113 as PyTorch extensions root...
Using /home/lclsg/.cache/torch_extensions/py37_cu113 as PyTorch extensions root...
Using /home/lclsg/.cache/torch_extensions/py37_cu113 as PyTorch extensions root...
Emitting ninja build file /home/lclsg/.cache/torch_extensions/py37_cu113/utils/build.ninja...
Building extension module utils...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module utils...
Time to load utils op: 0.5770063400268555 seconds
Loading extension module utils...
Time to load utils op: 0.5039846897125244 seconds
Loading extension module utils...
Loading extension module utils...
Time to load utils op: 0.6050903797149658 seconds
Loading extension module utils...
Time to load utils op: 0.6046648025512695 seconds
Time to load utils op: 0.60465407371521 seconds
Loading extension module utils...
Time to load utils op: 0.6044037342071533 seconds
Loading extension module utils...
Time to load utils op: 0.5047626495361328 seconds
Loading extension module utils...
Time to load utils op: 0.6048591136932373 seconds
[2022-08-17 21:31:30,258] [INFO] [stage3.py:567:_setup_for_real_optimizer] optimizer state initialized
Using /home/lclsg/.cache/torch_extensions/py37_cu113 as PyTorch extensions root...
Using /home/lclsg/.cache/torch_extensions/py37_cu113 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.0003268718719482422 seconds
Time to load utils op: 0.0003170967102050781 seconds
Using /home/lclsg/.cache/torch_extensions/py37_cu113 as PyTorch extensions root...
Using /home/lclsg/.cache/torch_extensions/py37_cu113 as PyTorch extensions root...
Using /home/lclsg/.cache/torch_extensions/py37_cu113 as PyTorch extensions root...
Using /home/lclsg/.cache/torch_extensions/py37_cu113 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.0004925727844238281 seconds
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.0005276203155517578 seconds
Using /home/lclsg/.cache/torch_extensions/py37_cu113 as PyTorch extensions root...
Time to load utils op: 0.0005223751068115234 seconds
Time to load utils op: 0.0005486011505126953 seconds
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.0005223751068115234 seconds
[2022-08-17 21:31:32,629] [INFO] [utils.py:828:see_memory_usage] After initializing ZeRO optimizer
[2022-08-17 21:31:32,630] [INFO] [utils.py:833:see_memory_usage] MA 0.93 GB         Max_MA 23.94 GB         CA 25.84 GB         Max_CA 26 GB 
[2022-08-17 21:31:32,630] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 296.69 GB, percent = 58.9%
[2022-08-17 21:31:32,630] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed Final Optimizer = AdamW
[2022-08-17 21:31:32,630] [INFO] [engine.py:795:_configure_lr_scheduler] DeepSpeed using client LR scheduler
[2022-08-17 21:31:32,630] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2022-08-17 21:31:32,631] [INFO] [logging.py:69:log_dist] [Rank 0] step=0, skipped=0, lr=[5e-05, 5e-05], mom=[(0.9, 0.999), (0.9, 0.999)]
[2022-08-17 21:31:32,631] [INFO] [config.py:1059:print] DeepSpeedEngine configuration:
[2022-08-17 21:31:32,631] [INFO] [config.py:1063:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2022-08-17 21:31:32,631] [INFO] [config.py:1063:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2022-08-17 21:31:32,631] [INFO] [config.py:1063:print]   amp_enabled .................. False
[2022-08-17 21:31:32,631] [INFO] [config.py:1063:print]   amp_params ................... False
[2022-08-17 21:31:32,632] [INFO] [config.py:1063:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": null, 
    "exps_dir": null, 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2022-08-17 21:31:32,632] [INFO] [config.py:1063:print]   bfloat16_enabled ............. False
[2022-08-17 21:31:32,632] [INFO] [config.py:1063:print]   checkpoint_tag_validation_enabled  True
[2022-08-17 21:31:32,632] [INFO] [config.py:1063:print]   checkpoint_tag_validation_fail  False
[2022-08-17 21:31:32,632] [INFO] [config.py:1063:print]   communication_data_type ...... None
[2022-08-17 21:31:32,632] [INFO] [config.py:1063:print]   curriculum_enabled ........... False
[2022-08-17 21:31:32,632] [INFO] [config.py:1063:print]   curriculum_params ............ False
[2022-08-17 21:31:32,632] [INFO] [config.py:1063:print]   dataloader_drop_last ......... False
[2022-08-17 21:31:32,632] [INFO] [config.py:1063:print]   disable_allgather ............ False
[2022-08-17 21:31:32,632] [INFO] [config.py:1063:print]   dump_state ................... False
[2022-08-17 21:31:32,632] [INFO] [config.py:1063:print]   dynamic_loss_scale_args ...... {'init_scale': 1024, 'scale_window': 1000, 'delayed_shift': 2, 'min_scale': 1}
[2022-08-17 21:31:32,632] [INFO] [config.py:1063:print]   eigenvalue_enabled ........... False
[2022-08-17 21:31:32,632] [INFO] [config.py:1063:print]   eigenvalue_gas_boundary_resolution  1
[2022-08-17 21:31:32,632] [INFO] [config.py:1063:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2022-08-17 21:31:32,632] [INFO] [config.py:1063:print]   eigenvalue_layer_num ......... 0
[2022-08-17 21:31:32,632] [INFO] [config.py:1063:print]   eigenvalue_max_iter .......... 100
[2022-08-17 21:31:32,632] [INFO] [config.py:1063:print]   eigenvalue_stability ......... 1e-06
[2022-08-17 21:31:32,632] [INFO] [config.py:1063:print]   eigenvalue_tol ............... 0.01
[2022-08-17 21:31:32,632] [INFO] [config.py:1063:print]   eigenvalue_verbose ........... False
[2022-08-17 21:31:32,632] [INFO] [config.py:1063:print]   elasticity_enabled ........... False
[2022-08-17 21:31:32,632] [INFO] [config.py:1063:print]   flops_profiler_config ........ {
    "enabled": false, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2022-08-17 21:31:32,632] [INFO] [config.py:1063:print]   fp16_enabled ................. True
[2022-08-17 21:31:32,632] [INFO] [config.py:1063:print]   fp16_master_weights_and_gradients  False
[2022-08-17 21:31:32,632] [INFO] [config.py:1063:print]   fp16_mixed_quantize .......... False
[2022-08-17 21:31:32,632] [INFO] [config.py:1063:print]   global_rank .................. 0
[2022-08-17 21:31:32,632] [INFO] [config.py:1063:print]   gradient_accumulation_steps .. 1
[2022-08-17 21:31:32,632] [INFO] [config.py:1063:print]   gradient_clipping ............ 0.0
[2022-08-17 21:31:32,632] [INFO] [config.py:1063:print]   gradient_predivide_factor .... 1.0
[2022-08-17 21:31:32,632] [INFO] [config.py:1063:print]   initial_dynamic_scale ........ 1024
[2022-08-17 21:31:32,632] [INFO] [config.py:1063:print]   loss_scale ................... 0
[2022-08-17 21:31:32,632] [INFO] [config.py:1063:print]   memory_breakdown ............. False
[2022-08-17 21:31:32,632] [INFO] [config.py:1063:print]   optimizer_legacy_fusion ...... False
[2022-08-17 21:31:32,632] [INFO] [config.py:1063:print]   optimizer_name ............... None
[2022-08-17 21:31:32,632] [INFO] [config.py:1063:print]   optimizer_params ............. None
[2022-08-17 21:31:32,632] [INFO] [config.py:1063:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2022-08-17 21:31:32,632] [INFO] [config.py:1063:print]   pld_enabled .................. False
[2022-08-17 21:31:32,632] [INFO] [config.py:1063:print]   pld_params ................... False
[2022-08-17 21:31:32,632] [INFO] [config.py:1063:print]   prescale_gradients ........... False
[2022-08-17 21:31:32,632] [INFO] [config.py:1063:print]   quantize_change_rate ......... 0.001
[2022-08-17 21:31:32,632] [INFO] [config.py:1063:print]   quantize_groups .............. 1
[2022-08-17 21:31:32,632] [INFO] [config.py:1063:print]   quantize_offset .............. 1000
[2022-08-17 21:31:32,632] [INFO] [config.py:1063:print]   quantize_period .............. 1000
[2022-08-17 21:31:32,632] [INFO] [config.py:1063:print]   quantize_rounding ............ 0
[2022-08-17 21:31:32,632] [INFO] [config.py:1063:print]   quantize_start_bits .......... 16
[2022-08-17 21:31:32,632] [INFO] [config.py:1063:print]   quantize_target_bits ......... 8
[2022-08-17 21:31:32,632] [INFO] [config.py:1063:print]   quantize_training_enabled .... False
[2022-08-17 21:31:32,632] [INFO] [config.py:1063:print]   quantize_type ................ 0
[2022-08-17 21:31:32,632] [INFO] [config.py:1063:print]   quantize_verbose ............. False
[2022-08-17 21:31:32,633] [INFO] [config.py:1063:print]   scheduler_name ............... None
[2022-08-17 21:31:32,633] [INFO] [config.py:1063:print]   scheduler_params ............. None
[2022-08-17 21:31:32,633] [INFO] [config.py:1063:print]   sparse_attention ............. None
[2022-08-17 21:31:32,633] [INFO] [config.py:1063:print]   sparse_gradients_enabled ..... False
[2022-08-17 21:31:32,633] [INFO] [config.py:1063:print]   steps_per_print .............. inf
[2022-08-17 21:31:32,633] [INFO] [config.py:1063:print]   tensorboard_enabled .......... False
[2022-08-17 21:31:32,633] [INFO] [config.py:1063:print]   tensorboard_job_name ......... DeepSpeedJobName
[2022-08-17 21:31:32,633] [INFO] [config.py:1063:print]   tensorboard_output_path ...... 
[2022-08-17 21:31:32,633] [INFO] [config.py:1063:print]   train_batch_size ............. 256
[2022-08-17 21:31:32,633] [INFO] [config.py:1063:print]   train_micro_batch_size_per_gpu  32
[2022-08-17 21:31:32,633] [INFO] [config.py:1063:print]   use_quantizer_kernel ......... False
[2022-08-17 21:31:32,633] [INFO] [config.py:1063:print]   wall_clock_breakdown ......... False
[2022-08-17 21:31:32,633] [INFO] [config.py:1063:print]   world_size ................... 8
[2022-08-17 21:31:32,633] [INFO] [config.py:1063:print]   zero_allow_untested_optimizer  True
[2022-08-17 21:31:32,633] [INFO] [config.py:1063:print]   zero_config .................. {
    "stage": 3, 
    "contiguous_gradients": true, 
    "reduce_scatter": true, 
    "reduce_bucket_size": 5.000000e+08, 
    "allgather_partitions": true, 
    "allgather_bucket_size": 5.000000e+08, 
    "overlap_comm": true, 
    "load_from_fp32_weights": true, 
    "elastic_checkpoint": false, 
    "offload_param": {
        "device": "cpu", 
        "nvme_path": null, 
        "buffer_count": 5, 
        "buffer_size": 1.000000e+08, 
        "max_in_cpu": 1.000000e+09, 
        "pin_memory": false
    }, 
    "offload_optimizer": {
        "device": "cpu", 
        "nvme_path": null, 
        "buffer_count": 4, 
        "pin_memory": false, 
        "pipeline_read": false, 
        "pipeline_write": false, 
        "fast_init": false, 
        "pipeline": false
    }, 
    "sub_group_size": 1.000000e+09, 
    "prefetch_bucket_size": 5.000000e+07, 
    "param_persistence_threshold": 1.000000e+05, 
    "max_live_parameters": 1.000000e+09, 
    "max_reuse_distance": 1.000000e+09, 
    "gather_16bit_weights_on_model_save": false, 
    "ignore_unused_parameters": true, 
    "round_robin_gradients": false, 
    "legacy_stage1": false
}
[2022-08-17 21:31:32,633] [INFO] [config.py:1063:print]   zero_enabled ................. True
[2022-08-17 21:31:32,633] [INFO] [config.py:1063:print]   zero_optimization_stage ...... 3
[2022-08-17 21:31:32,633] [INFO] [config.py:1071:print]   json = {
    "train_batch_size": 256, 
    "train_micro_batch_size_per_gpu": 32, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 3, 
        "offload_optimizer": {
            "device": "cpu"
        }, 
        "offload_param": {
            "device": "cpu"
        }, 
        "stage3_gather_16bit_weights_on_model_save": false
    }, 
    "steps_per_print": inf, 
    "fp16": {
        "enabled": true, 
        "initial_scale_power": 10
    }, 
    "zero_allow_untested_optimizer": true
}
Using /home/lclsg/.cache/torch_extensions/py37_cu113 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.00031495094299316406 seconds
08/17/2022 21:31:32 - INFO - __main__ - ***** Running training *****
08/17/2022 21:31:32 - INFO - __main__ -   Num examples = 2355
08/17/2022 21:31:32 - INFO - __main__ -   Num Epochs = 3
08/17/2022 21:31:32 - INFO - __main__ -   Instantaneous batch size per device = 32
08/17/2022 21:31:32 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 256
08/17/2022 21:31:32 - INFO - __main__ -   Gradient Accumulation steps = 1
08/17/2022 21:31:32 - INFO - __main__ -   Total optimization steps = 30

  0%|          | 0/30 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...

  3%|▎         | 1/30 [00:38<18:49, 38.93s/it]08/17/2022 21:32:11 - INFO - __main__ - max gpu allocated: 39.53339862823486 GB, max gpu reserved: 75.40234375 GB
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
[2022-08-17 21:32:51,384] [WARNING] [stage3.py:2399:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding torch.cuda.empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  7%|▋         | 2/30 [01:18<18:24, 39.45s/it]08/17/2022 21:32:51 - INFO - __main__ - max gpu allocated: 41.88211727142334 GB, max gpu reserved: 77.013671875 GB
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...

 10%|█         | 3/30 [01:56<17:20, 38.53s/it]08/17/2022 21:33:28 - INFO - __main__ - max gpu allocated: 41.88211727142334 GB, max gpu reserved: 77.013671875 GB
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...

 13%|█▎        | 4/30 [02:33<16:27, 38.00s/it]08/17/2022 21:34:06 - INFO - __main__ - max gpu allocated: 41.88211727142334 GB, max gpu reserved: 77.013671875 GB
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...

 17%|█▋        | 5/30 [03:10<15:42, 37.69s/it]08/17/2022 21:34:43 - INFO - __main__ - max gpu allocated: 41.88211727142334 GB, max gpu reserved: 77.013671875 GB
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...

 20%|██        | 6/30 [03:47<15:01, 37.56s/it]08/17/2022 21:35:20 - INFO - __main__ - max gpu allocated: 41.88211727142334 GB, max gpu reserved: 77.013671875 GB
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...

 23%|██▎       | 7/30 [04:24<14:18, 37.35s/it]08/17/2022 21:35:57 - INFO - __main__ - max gpu allocated: 41.88211727142334 GB, max gpu reserved: 77.013671875 GB
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...

 27%|██▋       | 8/30 [05:01<13:38, 37.21s/it]08/17/2022 21:36:34 - INFO - __main__ - max gpu allocated: 41.88211727142334 GB, max gpu reserved: 77.013671875 GB
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...

 30%|███       | 9/30 [05:38<13:01, 37.21s/it]08/17/2022 21:37:11 - INFO - __main__ - max gpu allocated: 41.88211727142334 GB, max gpu reserved: 77.013671875 GB
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...

 33%|███▎      | 10/30 [06:15<12:22, 37.15s/it]08/17/2022 21:37:48 - INFO - __main__ - max gpu allocated: 41.88211727142334 GB, max gpu reserved: 77.013671875 GB
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...

 37%|███▋      | 11/30 [06:53<11:46, 37.16s/it]08/17/2022 21:38:25 - INFO - __main__ - max gpu allocated: 41.88211727142334 GB, max gpu reserved: 77.013671875 GB
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...

 40%|████      | 12/30 [07:30<11:08, 37.12s/it]08/17/2022 21:39:02 - INFO - __main__ - max gpu allocated: 41.88211727142334 GB, max gpu reserved: 77.013671875 GB
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...

 43%|████▎     | 13/30 [08:07<10:30, 37.09s/it]08/17/2022 21:39:39 - INFO - __main__ - max gpu allocated: 41.88211727142334 GB, max gpu reserved: 77.013671875 GB
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...

 47%|████▋     | 14/30 [08:44<09:53, 37.09s/it]08/17/2022 21:40:16 - INFO - __main__ - max gpu allocated: 41.88211727142334 GB, max gpu reserved: 77.013671875 GB
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...

 50%|█████     | 15/30 [09:21<09:15, 37.06s/it]08/17/2022 21:40:53 - INFO - __main__ - max gpu allocated: 41.88211727142334 GB, max gpu reserved: 77.013671875 GB
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...

 53%|█████▎    | 16/30 [09:58<08:39, 37.08s/it]08/17/2022 21:41:30 - INFO - __main__ - max gpu allocated: 41.88211727142334 GB, max gpu reserved: 77.013671875 GB
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...

 57%|█████▋    | 17/30 [10:35<08:01, 37.03s/it]08/17/2022 21:42:07 - INFO - __main__ - max gpu allocated: 41.88211727142334 GB, max gpu reserved: 77.013671875 GB
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...

 60%|██████    | 18/30 [11:12<07:24, 37.04s/it]08/17/2022 21:42:44 - INFO - __main__ - max gpu allocated: 41.88211727142334 GB, max gpu reserved: 77.013671875 GB
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...

 63%|██████▎   | 19/30 [11:49<06:47, 37.08s/it]08/17/2022 21:43:22 - INFO - __main__ - max gpu allocated: 41.88211727142334 GB, max gpu reserved: 77.013671875 GB
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...

 67%|██████▋   | 20/30 [12:26<06:10, 37.03s/it]08/17/2022 21:43:58 - INFO - __main__ - max gpu allocated: 41.88211727142334 GB, max gpu reserved: 77.013671875 GB
08/17/2022 21:43:58 - INFO - __main__ - step time = 37.05007975101471 s

